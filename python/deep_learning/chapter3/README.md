# 3章 ニューラルネットワーク

パーセプトロンでは論理回路の重ね合わせで複雑な関数でも表現することが可能だった。

一方で関数を決定するための重みやバイアスは人間の手で決める必要がある。

ニューラルネットワークはこの作業をデータから自動で学習できるという性質を持つため、この問題の解決方法となりうる。

## 活性化関数

パーセプトロンを表現する関数

$$
y = \left\{
\begin{array}{ll}
0 & (b + x1w1 + x2w2 \leq 0) \\
1 & (b + x1w1 + x2w2 \gt 0)
\end{array}
\right.
$$

をより単純な形に書き換える。

$$
y = h(a)
$$

とし、関数$h$とその引数$a$をそれぞれ以下のように定義する。

$$
a = b + w1x1 + w2x2
$$

$$
h(x) = \left\{
\begin{array}{ll}
0 & (x \leq 0) \\
1 & (x \gt 0)
\end{array}
\right.
$$

この時$h(x)$を入力信号の総和がどのように活性化（発火）されるかを示すため、活性化関数と呼ぶ。

パーセプトロンでは活性化関数に上記のようなステップ関数を利用しているが、ステップ関数以外の関数を利用することもできる。

ステップ関数の実装はstep_function.pyで実施

## シグモイド関数

ニューラルネットワークでは活性化関数として以下のようなシグモイド関数が用いられる。

$$
h(x) = \frac{1}{1 + e^{-x}}
$$

シグモイド関数の実装はsigmoid.pyで実施

## ReLU関数

シグモイド関数は古くからニューラルネットワークに利用されてきだが、最近ではReLU関数を利用するらしい。

$$
h(x) = \left\{
\begin{array}{ll}
0 & (x \leq 0) \\
x & (x \gt 0)
\end{array}
\right.
$$

ReLU関数の実装はReLU.pyで実施

## 多次元配列の計算

ニューラルネットワークの実装を効率的に行うために、多次元配列の計算方法についてまとめる。

### 多次元配列の生成

numpyの関数を使える

```python
# 作成
numpy.array
# 次元の取得
numpy.ndim(array)
# 配列の形状
numpy.shape
```

### 行列の内積

行列の内積は

```python
# 内積
numpy.dot(A, B)
```

で計算可能。この際左側行列の行数（`A.shape`の1要素目）と右側行列の列数（`B.shape`の0要素目）は一致していないとエラーになる。

まあ当然。

### ニューラルネットワークの内積

バイアスと活性化関数を省略し、重みだけがある簡単なニューラルネットワークの実装を行う。

入力行列（各入力値を要素にもつ行列）を**X**、重み行列を**W**、出力行列を**Y**をするニューラルネットワークは、内積を利用して簡単に計算することができる。

```python
# 入力行列X、重み行列W、出力行列Y
Y = numpy.dot(X, W)
```

作用させる順番に注意が必要

### ニューラルネットワークの実装

入力層2つ、隠し層1が3つ、隠し層2が2つ、出力層が2つのニューロンで形成されたニュラルネットワークを考える。

各層での計算は

1. 重み付き和の計算

1. 活性化関数でによる変換

の手順で進行する。

隠し層1の1番目のニューロンへの信号伝達を式にすると

$$
a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1
$$

と表現できる。

行列の内積を使うと1層目の重み付き和は

$$
A^{(1)} = XW^{(1)} + B^{(1)}
$$

ただし

$$
A^{(1)} =
\begin{pmatrix}
a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
\end{pmatrix},
X=
\begin{pmatrix}
x_1 & x_2
\end{pmatrix},
B^{(1)}=
\begin{pmatrix}
b_1^{(1)} & b_2^{(1)} & b_3^{(1)}
\end{pmatrix}
$$

$$
W^{(1)}=
\begin{pmatrix}
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}
$$

とする。

以前pythonで作成した関数はnumpy行列を引数に取れるように実装しているので、計算した重み付き和をそのまま任意の活性化関数にかけることができる。

2層目以降でも同様の処理を行うことでニューラルネットワークを用いた計算を実行することができる。

network.pyにsigmoid関数を利用した場合とReLUを利用した場合の実装を行った。

### 出力層の設計

ニューラルネットワークは分類問題と回帰問題の両方に用いることができるが、どちらに用いるかによって出力層の活性化関数を変更する必要がある。

一般的に回帰問題では恒等関数を、分類問題ではソフトマックス関数を利用する。

#### 恒等関数とソフトマックス関数

恒等関数が入力値をそのまま出力する（何もしない）関数であるのに対し、ソフトマックス関数は以下の式で表現される。

$$
y_k = \frac{e^{a_k}}{\sum_{i=1}^n e^{a_i}}
$$

分母が示すように、ソフトマックス関数を用いた出力は全ての入力の影響を受ける。

network.pyのidentity_fanctionをソフトマックス関数用に修正したsoftmax_functionを作成し、結果を比較した。

実装の際に入力信号の最大値Cを引くことで、指数同士の除算によるオーバーフローを防ぐようにした。

#### ソフトマックス関数の性質

ソフトマックス関数の重要な性質は**出力値の総和が1になる**ことである。

この性質により、各ニューロンの出力結果を確率として解釈することが可能となる。

例えば

$$
\left(0.47781171, 0.52218829\right)
$$

という出力を得た場合、y[0]の確率が0.478(47.8%)、y[1]の確率が0.522(52.2%)と解釈できる。

この性質によって、ソフトマックス関数を用いると、問題に対して確率的（統計的）な対応を行うことができる。

一般的なニューラルネットワークのクラス分類では、出力のもっとも大きいニューロンの結果を認識結果とする。ソフトマックス関数をかけても出力の大小関係自体は変わらないうえ、指数の計算はそれなりに計算量が大きくなるため、実際の問題では出力層のソフトマックス関数は省略するのが一般的である。

#### 出力層のニューロンの数

出力層のニューロンの数は解くべき問題に応じて適宜決める必要がある。

例えば入力画像にに対して、その画像が数字の0から9のどれかを予測する問題（10クラス分類問題）ではニューロンは10個に設定する。