# 3章 ニューラルネットワーク

パーセプトロンでは論理回路の重ね合わせで複雑な関数でも表現することが可能だった。

一方で関数を決定するための重みやバイアスは人間の手で決める必要がある。

ニューラルネットワークはこの作業をデータから自動で学習できるという性質を持つため、この問題の解決方法となりうる。

## 活性化関数

パーセプトロンを表現する関数

$$
y = \left\{
\begin{array}{ll}
0 & (b + x1w1 + x2w2 \leq 0) \\
1 & (b + x1w1 + x2w2 \gt 0)
\end{array}
\right.
$$

をより単純な形に書き換える。

$$
y = h(a)
$$

とし、関数$h$とその引数$a$をそれぞれ以下のように定義する。

$$
a = b + w1x1 + w2x2
$$

$$
h(x) = \left\{
\begin{array}{ll}
0 & (x \leq 0) \\
1 & (x \gt 0)
\end{array}
\right.
$$

この時$h(x)$を入力信号の総和がどのように活性化（発火）されるかを示すため、活性化関数と呼ぶ。

パーセプトロンでは活性化関数に上記のようなステップ関数を利用しているが、ステップ関数以外の関数を利用することもできる。

ステップ関数の実装はstep_function.pyで実施

## シグモイド関数

ニューラルネットワークでは活性化関数として以下のようなシグモイド関数が用いられる。

$$
h(x) = \frac{1}{1 + e^{-x}}
$$

シグモイド関数の実装はsigmoid.pyで実施

## ReLU関数

シグモイド関数は古くからニューラルネットワークに利用されてきだが、最近ではReLU関数を利用するらしい。

$$
h(x) = \left\{
\begin{array}{ll}
0 & (x \leq 0) \\
x & (x \gt 0)
\end{array}
\right.
$$

ReLU関数の実装はReLU.pyで実施

## 多次元配列の計算

ニューラルネットワークの実装を効率的に行うために、多次元配列の計算方法についてまとめる。

### 多次元配列の生成

numpyの関数を使える

```python
# 作成
numpy.array
# 次元の取得
numpy.ndim(array)
# 配列の形状
numpy.shape
```

### 行列の内積

行列の内積は

```python
# 内積
numpy.dot(A, B)
```

で計算可能。この際左側行列の行数（`A.shape`の1要素目）と右側行列の列数（`B.shape`の0要素目）は一致していないとエラーになる。

まあ当然。

### ニューラルネットワークの内積

バイアスと活性化関数を省略し、重みだけがある簡単なニューラルネットワークの実装を行う。

入力行列（各入力値を要素にもつ行列）を**X**、重み行列を**W**、出力行列を**Y**をするニューラルネットワークは、内積を利用して簡単に計算することができる。

```python
# 入力行列X、重み行列W、出力行列Y
Y = numpy.dot(X, W)
```

作用させる順番に注意が必要

### ニューラルネットワークの実装

入力層2つ、隠し層1が3つ、隠し層2が2つ、出力層が2つのニューロンで形成されたニュラルネットワークを考える。

各層での計算は

1. 重み付き和の計算

1. 活性化関数でによる変換

の手順で進行する。

隠し層1の1番目のニューロンへの信号伝達を式にすると

$$
a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1
$$

と表現できる。

行列の内積を使うと1層目の重み付き和は

$$
A^{(1)} = XW^{(1)} + B^{(1)}
$$

ただし

$$
A^{(1)} =
\begin{pmatrix}
a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
\end{pmatrix},
X=
\begin{pmatrix}
x_1 & x_2
\end{pmatrix},
B^{(1)}=
\begin{pmatrix}
b_1^{(1)} & b_2^{(1)} & b_3^{(1)}
\end{pmatrix}
$$

$$
W^{(1)}=
\begin{pmatrix}
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}
$$

と表現できるので内積を使って重み付き和を計算する。

以前pythonで作成した関数はnumpy行列を引数に取れるように実装しているので、計算した重み付き和をそのまま任意の活性化関数にかけることができる。

2層目以降でも同様の処理を行うことでニューラルネットワークを用いた計算を実行することができる。

network.pyにsigmoid関数を利用した場合とReLUを利用した場合の実装を行った。