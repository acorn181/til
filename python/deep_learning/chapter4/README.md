# 4章 ニューラルネットワークの学習

訓練データから最適な重みパラメータの値を自動で獲得する「学習」を行えるようにするために、損失関数という「指標」を導入する。

この章ではできるだけ小さな損失関数の値を導き出す手法として、勾配法を扱う

## 損失関数

ニューラルネットワークの学習では、損失関数（loss function）によって現在の状態を表現する。

この損失関数には任意の関数を利用することができるが、一般には2乗和誤差や交差エントロピー誤差などが用いられる。

### 2乗和誤差

2乗和誤差(mean squared error)は以下の数式で表現される。

$$
E = \frac{1}{2}\sum_{k}{\left(y_k - t_k\right)^2}
$$

ここで$y_k$はニューラルネットワークの出力、$t_k$は教師データを表し、$k$はデータの次元を表す。

出力結果が教師データにより適合している場合、2乗和誤差の計算結果はより小さくなる。

### 交差エントロピー誤差

交差エントロピー誤差(cross entropy error)は以下の数式で定義される。

$$
E = -\sum_{k}{t_k\log y_k}
$$

$y_k$はニューラルネットワークの出力、$t_k$はone-hot 表現の正解ラベル（正解値のみ1でそのほかは0の配列）とすると、この式は実質的に正解ラベルが1に対応する出力値のみを計算することになる。

### ミニバッチ学習

機械学習の問題では訓練データを使って学習を行う。

これは、訓練データに対する損失関数を計算し、その値をできるだけ小さくするようなパラメータを探し出すということなので、損失関数は全ての訓練データを対象として求める必要がある。

100個の訓練データがある場合には100個の損失関数の和を指標とする。

$$
E = -\frac{1}{N}\sum_{n}\sum_{k}{t_{nk}\log y_{nk}}
$$

N個のデータがある場合のn番目の出力に対する損失関数を計算し、最後にNで割ることで正規化している。

現実的には、大量の訓練データ全てに対して評価を行うのは時間がかかりすぎて現実的ではないため、データの中から一部を取り出して利用する。

この手法をミニバッチ学習と呼ぶ。

numpyでは`numpy.random.choice()`を使って以下のようなミニバッチデータを作成することができる。

```python
import numpy as np
# 訓練データ
print(x_train.shape) # (60000, 784)
# 教師データ
print(t_train.shape) # (60000, 10)

train_size = x_train.shape[0]
batch_size = 100
batch_mask = np.random_choice(train_size, batch_size)

x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

### なぜ損失関数を設定するのか

評価指標として認識精度を利用してはダメなのか。

ニューラルネットワークの学習では、損失関数をより小さくするためにパラメータの勾配を計算し、その値を基にパラメータの値を変化させていく。

ここで損失関数の代わりに認識精度を利用した場合、一つのパラメータを微小に変化させただけでは精度はほとんど変化しないため、勾配が実質的に0になってしまう。

また、もし変化があっても認識精度の値は不連続な変化をするため、勾配の計算がうまく行えなくなってしまう。

## 数値微分

数値微分を行うためには以下の2点に注意が必要

- 丸め誤差を考慮して、微小差分の値は$10^{-4}$程度にする

- 誤差を減らす工夫として、中心差分（$(x + h)$と$(x - h)$での関数$f$の差分）を計算する

複数変数がある場合（偏微分）にも各変数ごとに数値微分を行えばよい。

## 勾配

各変数に対する偏微分をまとめたものを勾配(gradient)と呼ぶ。

サンプル実装はnumerical_grad.py

### 勾配法

勾配法では、現在の場所から勾配方向に一定の距離移動するのを繰り返すことで関数の値を徐々に減らしていく。

これを数式で表すと以下のようになる。

$$
x_0 = x_0 - \eta\frac{\partial f}{\partial x_0}
$$

$$
x_1 = x_1 - \eta\frac{\partial f}{\partial x_1}
$$

ここで$\eta$は学習率(learning rate)と呼ばれ、1回の学習でどれだけ学習すべきか、どれだけパラメータを更新するかを決める。

### シンプルなニューラルネットワークでの勾配法の実装

簡単なニュラルネットワークを使って勾配を求める実装を行う。